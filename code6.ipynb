{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8lge4rdNlSW",
        "outputId": "f342218e-bd0a-432a-e763-c1536580a445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (1.0.9)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.12/dist-packages (2.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\n",
        "!pip install transformers sentence-transformers langdetect chromadb sentencepiece torch babel nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Optional, List, Dict, Any\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\") # Download punkt_tab for multilingual tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from langdetect import detect, DetectorFactory\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "SUPPORTED_LANGUAGES = {\n",
        "    \"en\": \"English\",\n",
        "    \"es\": \"Spanish\",\n",
        "    \"hi\": \"Hindi\",\n",
        "    \"zh\": \"Chinese (Simplified)\"\n",
        "}\n",
        "\n",
        "INTERNAL_LANG = \"en\"\n",
        "DEVICE = \"cpu\" # Changed device to CPU\n",
        "\n",
        "# Translation models for multilingual support (Helsinki/Marian)\n",
        "TRANSLATION_MODELS = {\n",
        "    (\"es\", \"en\"): \"Helsinki-NLP/opus-mt-es-en\",\n",
        "    (\"en\", \"es\"): \"Helsinki-NLP/opus-mt-en-es\",\n",
        "    (\"hi\", \"en\"): \"Helsinki-NLP/opus-mt-hi-en\",\n",
        "    (\"en\", \"hi\"): \"Helsinki-NLP/opus-mt-en-hi\",\n",
        "    (\"zh\", \"en\"): \"Helsinki-NLP/opus-mt-zh-en\",\n",
        "    (\"en\", \"zh\"): \"Helsinki-NLP/opus-mt-en-zh\",\n",
        "}\n",
        "\n",
        "# Cultural info\n",
        "CULTURE_MAP = {\n",
        "    \"en\": {\"greeting\": \"Hello\"},\n",
        "    \"es\": {\"greeting\": \"Hola\"},\n",
        "    \"hi\": {\"greeting\": \"नमस्ते\"},\n",
        "    \"zh\": {\"greeting\": \"你好\"},\n",
        "}\n",
        "\n",
        "# ---------- Global caches ----------\n",
        "_translation_pipelines = {}\n",
        "_embedding_model = None\n",
        "_sentiment_analyzer = None\n",
        "\n",
        "# ---------- Helper Classes ----------\n",
        "@dataclass\n",
        "class TranslationPipeline:\n",
        "    src: str\n",
        "    tgt: str\n",
        "    model_name: str\n",
        "    tokenizer: Any\n",
        "    model: Any\n",
        "    pipeline: Any\n",
        "\n",
        "# ---------- Functions ----------\n",
        "\n",
        "# Language detection\n",
        "def detect_language(text: str) -> str:\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "        if lang.startswith(\"zh\"):\n",
        "            return \"zh\"\n",
        "        return lang if lang in SUPPORTED_LANGUAGES else \"en\"\n",
        "    except:\n",
        "        return \"en\"\n",
        "\n",
        "# Translation\n",
        "def load_translation_pipeline(src: str, tgt: str) -> TranslationPipeline:\n",
        "    key = (src, tgt)\n",
        "    if key in _translation_pipelines:\n",
        "        return _translation_pipelines[key]\n",
        "    model_name = TRANSLATION_MODELS.get(key)\n",
        "    if not model_name:\n",
        "        raise ValueError(f\"No translation model for {src}->{tgt}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    from transformers import pipeline as hf_pipeline\n",
        "    # Explicitly set device to -1 for CPU\n",
        "    pipe = hf_pipeline(\"translation\", model=model, tokenizer=tokenizer, device=-1)\n",
        "    tp = TranslationPipeline(src, tgt, model_name, tokenizer, model, pipe)\n",
        "    _translation_pipelines[key] = tp\n",
        "    return tp\n",
        "\n",
        "def translate_text(text: str, src: str, tgt: str) -> str:\n",
        "    if src == tgt:\n",
        "        return text\n",
        "    tp = load_translation_pipeline(src, tgt)\n",
        "    sentences = sent_tokenize(text)\n",
        "    out = []\n",
        "    batch = []\n",
        "    for s in sentences:\n",
        "        batch.append(s)\n",
        "        if len(batch) >= 20:\n",
        "            res = tp.pipeline(\" \".join(batch), max_length=1000)\n",
        "            out.append(res[0][\"translation_text\"])\n",
        "            batch = []\n",
        "    if batch:\n",
        "        res = tp.pipeline(\" \".join(batch), max_length=1000)\n",
        "        out.append(res[0][\"translation_text\"])\n",
        "    return \" \".join(out)\n",
        "\n",
        "# Embedding model\n",
        "def get_embedding_model():\n",
        "    global _embedding_model\n",
        "    if _embedding_model is None:\n",
        "        # Explicitly set device to 'cpu'\n",
        "        _embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\", device='cpu')\n",
        "    return _embedding_model\n",
        "\n",
        "# Sentiment analysis\n",
        "def get_sentiment_analyzer():\n",
        "    global _sentiment_analyzer\n",
        "    if _sentiment_analyzer is None:\n",
        "        # Explicitly set device to -1 for CPU\n",
        "        _sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", device=-1)\n",
        "    return _sentiment_analyzer\n",
        "\n",
        "def analyze_sentiment(text: str) -> str:\n",
        "    analyzer = get_sentiment_analyzer()\n",
        "    result = analyzer(text)[0]\n",
        "    label = result[\"label\"]\n",
        "    # Map to simple positive/neutral/negative\n",
        "    if label in [\"1 star\", \"2 stars\"]:\n",
        "        return \"negative\"\n",
        "    elif label == \"3 stars\":\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return \"positive\"\n",
        "\n",
        "# Culturalization\n",
        "def culturalize_reply(reply_en: str, user_lang: str, sentiment: Optional[str] = None) -> str:\n",
        "    greeting = CULTURE_MAP.get(user_lang, CULTURE_MAP[\"en\"])[\"greeting\"]\n",
        "    # Adjust response tone based on sentiment\n",
        "    if sentiment == \"negative\":\n",
        "        reply_en = f\"I'm sorry to hear that. {reply_en}\"\n",
        "    elif sentiment == \"positive\":\n",
        "        reply_en = f\"Great! {reply_en}\"\n",
        "    return f\"{greeting}! {reply_en}\"\n",
        "\n",
        "# ---------- RAG retrieval stub ----------\n",
        "def query_knowledge_base_stub(query_en: str, top_k: int = 5) -> List[Dict]:\n",
        "    return []\n",
        "\n",
        "# ---------- LLM call stub ----------\n",
        "def call_llm(prompt_en: str, context_chunks: List[Dict], user_lang: str) -> str:\n",
        "    ctx = \"\\n\".join([c.get(\"document\",\"\") for c in context_chunks])\n",
        "    return f\"Answering: {prompt_en}\\nContext:\\n{ctx}\"\n",
        "\n",
        "# ---------- Main multilingual + sentiment pipeline ----------\n",
        "def process_user_message_with_sentiment(user_text: str,\n",
        "                                        user_lang: Optional[str] = None,\n",
        "                                        top_k: int = 5,\n",
        "                                        retrieval_fn = None,\n",
        "                                        llm_fn = None) -> Dict[str, Any]:\n",
        "\n",
        "    lang = user_lang or detect_language(user_text)\n",
        "    if lang not in SUPPORTED_LANGUAGES:\n",
        "        lang = \"en\"\n",
        "\n",
        "    # Translate input to internal language\n",
        "    if lang != INTERNAL_LANG:\n",
        "        user_text_en = translate_text(user_text, src=lang, tgt=INTERNAL_LANG)\n",
        "    else:\n",
        "        user_text_en = user_text\n",
        "\n",
        "    # Sentiment detection\n",
        "    sentiment = analyze_sentiment(user_text_en)\n",
        "\n",
        "    # Retrieve context\n",
        "    retrieval_fn = retrieval_fn or query_knowledge_base_stub\n",
        "    context_chunks = retrieval_fn(user_text_en, top_k=top_k)\n",
        "\n",
        "    # Call LLM\n",
        "    llm_fn = llm_fn or call_llm\n",
        "    reply_en = llm_fn(user_text_en, context_chunks, user_lang)\n",
        "\n",
        "    # Culturalize reply + sentiment adaptation\n",
        "    reply_cultur = culturalize_reply(reply_en, lang, sentiment)\n",
        "\n",
        "    # Translate back if needed\n",
        "    if lang != INTERNAL_LANG:\n",
        "        try:\n",
        "            reply_translated = translate_text(reply_cultur, src=INTERNAL_LANG, tgt=lang)\n",
        "        except Exception:\n",
        "            reply_translated = reply_cultur\n",
        "    else:\n",
        "        reply_translated = reply_cultur\n",
        "\n",
        "    return {\n",
        "        \"detected_language\": lang,\n",
        "        \"sentiment\": sentiment,\n",
        "        \"translated_user_text\": user_text_en,\n",
        "        \"reply_translated\": reply_translated,\n",
        "        \"reply_internal\": reply_en,\n",
        "        \"reply_localized\": reply_cultur\n",
        "    }\n",
        "\n",
        "# ---------- Demo ----------\n",
        "if __name__ == \"__main__\":\n",
        "    examples = [\n",
        "        \"I love this product! It's amazing.\",\n",
        "        \"I'm frustrated because the service is slow.\",\n",
        "        \"The chatbot works okay, nothing special.\",\n",
        "        \"Me encanta este servicio.\",\n",
        "        \"यह सेवा बहुत खराब है।\"\n",
        "    ]\n",
        "\n",
        "    for msg in examples:\n",
        "        out = process_user_message_with_sentiment(msg)\n",
        "        print(\"=\"*50)\n",
        "        print(f\"User: {msg}\")\n",
        "        print(f\"Detected Language: {out['detected_language']}\")\n",
        "        print(f\"Sentiment: {out['sentiment']}\")\n",
        "        print(f\"Chatbot Reply:\\n{out['reply_translated']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB2fH59wVzol",
        "outputId": "cb7b9615-cf46-45c4-8d28-68b8856dcc42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "User: I love this product! It's amazing.\n",
            "Detected Language: en\n",
            "Sentiment: positive\n",
            "Chatbot Reply:\n",
            "Hello! Great! Answering: I love this product! It's amazing.\n",
            "Context:\n",
            "\n",
            "\n",
            "==================================================\n",
            "User: I'm frustrated because the service is slow.\n",
            "Detected Language: en\n",
            "Sentiment: negative\n",
            "Chatbot Reply:\n",
            "Hello! I'm sorry to hear that. Answering: I'm frustrated because the service is slow.\n",
            "Context:\n",
            "\n",
            "\n",
            "==================================================\n",
            "User: The chatbot works okay, nothing special.\n",
            "Detected Language: en\n",
            "Sentiment: neutral\n",
            "Chatbot Reply:\n",
            "Hello! Answering: The chatbot works okay, nothing special.\n",
            "Context:\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "User: Me encanta este servicio.\n",
            "Detected Language: es\n",
            "Sentiment: positive\n",
            "Chatbot Reply:\n",
            "¡Hola! ¡Genial! Respuesta: Me encanta este servicio. Contexto:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "User: यह सेवा बहुत खराब है।\n",
            "Detected Language: hi\n",
            "Sentiment: negative\n",
            "Chatbot Reply:\n",
            "SURARSARS! मुझे लगता है कि सुनने के लिए खेद है. उत्तर: यह सेवा बहुत बुरा है. कॉन्टेक्स्ट:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3iu-fOKYOzK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}